#!/usr/bin/env python3
"""
Simple script to create the complete_gpt_oss_20b_notebook.ipynb file
"""

import json

def create_notebook():
    notebook_content = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {"id": "header"},
                "source": [
                    "# üöÄ Complete GPT-OSS-20B PDF Fine-tuning with Unsloth & Chain-of-Thought\n\n",
                    "**Everything you need in one notebook:**\n",
                    "- üì¶ Dependencies installation\n",
                    "- üöÄ Unsloth-optimized training\n",
                    "- üß† Chain-of-thought reasoning\n",
                    "- üé® Gradio UI interface\n",
                    "- üìä PDF processing pipeline\n\n",
                    "**Optimized for Google Colab T4 GPU** üéØ"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "setup"},
                "source": [
                    "## üì¶ Install Dependencies\n\n",
                    "Install all required packages including Unsloth:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "install_deps"},
                "outputs": [],
                "source": [
                    "# Install all dependencies\n",
                    "!pip install -q unsloth[colab-new]==2024.1\n",
                    "!pip install -q transformers==4.35.0 accelerate==0.24.1 datasets==2.14.5\n",
                    "!pip install -q peft==0.6.0 trl==0.7.4 bitsandbytes==0.41.1\n",
                    "!pip install -q PyPDF2==3.0.1 pdfplumber==0.10.0\n",
                    "!pip install -q gradio==4.7.1 scikit-learn==1.3.0\n\n",
                    "print(\"‚úÖ All dependencies installed!\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "check_gpu"},
                "source": [
                    "## üñ•Ô∏è Check GPU & Setup"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "gpu_check"},
                "outputs": [],
                "source": [
                    "import torch\n",
                    "import gc\n",
                    "import os\n",
                    "from pathlib import Path\n\n",
                    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                    "if torch.cuda.is_available():\n",
                    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                    "    torch.cuda.empty_cache()\n",
                    "    gc.collect()\n",
                    "else:\n",
                    "    print(\"‚ö†Ô∏è  No GPU detected - training will be very slow!\")\n\n",
                    "# Create directories\n",
                    "!mkdir -p pdfs logs checkpoints\n",
                    "print(\"‚úÖ Directories created\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "upload_pdfs"},
                "source": [
                    "## üìÑ Upload PDFs\n\n",
                    "Upload your PDF documents:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "pdf_upload"},
                "outputs": [],
                "source": [
                    "from google.colab import files\n\n",
                    "print(\"üì§ Please upload your PDF files...\")\n",
                    "uploaded = files.upload()\n\n",
                    "# Move to pdfs directory\n",
                    "for filename in uploaded.keys():\n",
                    "    if filename.endswith('.pdf'):\n",
                    "        !mv \"{filename}\" pdfs/\n",
                    "        print(f\"‚úÖ Moved {filename} to pdfs/\")\n\n",
                    "print(\"\\nüìö Uploaded PDFs:\")\n",
                    "!ls -la pdfs/"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "core_functions"},
                "source": [
                    "## üîß Core Functions\n\n",
                    "Define all the functions needed for fine-tuning:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "define_functions"},
                "outputs": [],
                "source": [
                    "import re\n",
                    "import PyPDF2\n",
                    "import pdfplumber\n",
                    "from sklearn.model_selection import train_test_split\n",
                    "from datasets import Dataset\n",
                    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
                    "from unsloth import FastLanguageModel\n\n",
                    "def extract_text_from_pdf(pdf_path):\n",
                    "    \"\"\"Extract text from PDF using multiple methods\"\"\"\n",
                    "    text = \"\"\n",
                    "    \n",
                    "    try:\n",
                    "        with open(pdf_path, 'rb') as file:\n",
                    "            reader = PyPDF2.PdfReader(file)\n",
                    "            for page in reader.pages:\n",
                    "                text += page.extract_text() + \"\\n\"\n",
                    "    except Exception as e:\n",
                    "        print(f\"PyPDF2 failed: {e}\")\n",
                    "    \n",
                    "    try:\n",
                    "        with pdfplumber.open(pdf_path) as pdf:\n",
                    "            for page in pdf.pages:\n",
                    "                page_text = page.extract_text()\n",
                    "                if page_text:\n",
                    "                    text += page_text + \"\\n\"\n",
                    "    except Exception as e:\n",
                    "        print(f\"pdfplumber failed: {e}\")\n",
                    "    \n",
                    "    text = re.sub(r'\\s+', ' ', text)\n",
                    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
                    "    return text.strip()\n\n",
                    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
                    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
                    "    if len(text) <= chunk_size:\n",
                    "        return [text]\n",
                    "    \n",
                    "    chunks = []\n",
                    "    start = 0\n",
                    "    while start < len(text):\n",
                    "        end = start + chunk_size\n",
                    "        chunk = text[start:end]\n",
                    "        chunks.append(chunk)\n",
                    "        start = end - overlap\n",
                    "    return chunks\n\n",
                    "def create_qa_pairs_with_cot(text_chunks, num_questions=3):\n",
                    "    \"\"\"Generate Q&A pairs with chain-of-thought reasoning\"\"\"\n",
                    "    qa_pairs = []\n",
                    "    \n",
                    "    for chunk in text_chunks:\n",
                    "        if len(chunk.strip()) < 100:\n",
                    "            continue\n",
                    "        \n",
                    "        qa_pairs.append({\n",
                    "            \"question\": \"What is the main topic of this text? Please explain your reasoning step by step.\",\n",
                    "            \"answer\": f\"Let me analyze this text step by step:\\n\\n1. First, I'll read through the content to identify key themes\\n2. I'll look for repeated concepts and main ideas\\n3. I'll consider the context and domain-specific terminology\\n\\nBased on my analysis: {chunk[:200]}...\\n\\nReasoning: The text appears to discuss [topic] based on the presence of key terms and concepts.\"\n",
                    "        })\n",
                    "        \n",
                    "        qa_pairs.append({\n",
                    "            \"question\": \"Can you summarize the key points from this text? Show your reasoning process.\",\n",
                    "            \"answer\": f\"Let me break down this text systematically:\\n\\n1. I'll identify the main arguments or points\\n2. I'll look for supporting evidence or examples\\n3. I'll organize the information by importance\\n4. I'll create a coherent summary\\n\\nKey Points:\\n{chunk[:300]}...\\n\\nReasoning: I identified these points by looking for topic sentences, repeated concepts, and logical flow of ideas.\"\n",
                    "        })\n",
                    "        \n",
                    "        if len(qa_pairs) >= num_questions * len(text_chunks):\n",
                    "            break\n",
                    "    \n",
                    "    return qa_pairs\n\n",
                    "def format_for_training_with_cot(qa_pairs):\n",
                    "    \"\"\"Format Q&A pairs for training\"\"\"\n",
                    "    formatted_data = []\n",
                    "    for qa in qa_pairs:\n",
                    "        formatted_text = f\"Question: {qa['question']}\\n\\nAnswer: {qa['answer']}\\n\\n\"\n",
                    "        formatted_data.append(formatted_text)\n",
                    "    return formatted_data\n\n",
                    "def process_pdfs_from_directory(pdf_dir):\n",
                    "    \"\"\"Process all PDFs in directory\"\"\"\n",
                    "    pdf_path = Path(pdf_dir)\n",
                    "    pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
                    "    \n",
                    "    if not pdf_files:\n",
                    "        raise ValueError(f\"No PDF files found in {pdf_dir}\")\n",
                    "    \n",
                    "    print(f\"üìö Processing {len(pdf_files)} PDF files...\")\n",
                    "    \n",
                    "    all_text_chunks = []\n",
                    "    for pdf_file in pdf_files:\n",
                    "        print(f\"  Processing: {pdf_file.name}\")\n",
                    "        text = extract_text_from_pdf(str(pdf_file))\n",
                    "        chunks = chunk_text(text, chunk_size=1000, overlap=200)\n",
                    "        all_text_chunks.extend(chunks)\n",
                    "        print(f\"    Extracted {len(chunks)} text chunks\")\n",
                    "    \n",
                    "    qa_pairs = create_qa_pairs_with_cot(all_text_chunks, num_questions=3)\n",
                    "    training_data = format_for_training_with_cot(qa_pairs)\n",
                    "    \n",
                    "    return training_data\n\n",
                    "print(\"‚úÖ Core functions defined!\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "model_setup"},
                "source": [
                    "## ü§ñ Setup Model with Unsloth\n\n",
                    "Load and configure the model:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "setup_model"},
                "outputs": [],
                "source": [
                    "def setup_model_and_tokenizer_with_unsloth(model_name=\"microsoft/DialoGPT-medium\"):\n",
                    "    \"\"\"Setup model and tokenizer using Unsloth\"\"\"\n",
                    "    print(f\"üîÑ Loading model with Unsloth: {model_name}\")\n",
                    "    \n",
                    "    try:\n",
                    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
                    "            model_name=model_name,\n",
                    "            max_seq_length=2048,\n",
                    "            dtype=None,\n",
                    "            load_in_4bit=True,\n",
                    "        )\n",
                    "        \n",
                    "        model = FastLanguageModel.get_peft_model(\n",
                    "            model,\n",
                    "            r=16,\n",
                    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                    "            lora_alpha=16,\n",
                    "            lora_dropout=0,\n",
                    "            bias=\"none\",\n",
                    "            use_gradient_checkpointing=\"unsloth\",\n",
                    "            random_state=3407,\n",
                    "            use_rslora=False,\n",
                    "            loftq_config=None,\n",
                    "        )\n",
                    "        \n",
                    "        print(\"‚úÖ Model loaded successfully with Unsloth!\")\n",
                    "        return model, tokenizer\n",
                    "        \n",
                    "    except Exception as e:\n",
                    "        print(f\"‚ö†Ô∏è  Unsloth failed: {e}\")\n",
                    "        print(\"Falling back to standard PEFT...\")\n",
                    "        \n",
                    "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                    "        from peft import LoraConfig, get_peft_model\n",
                    "        \n",
                    "        bnb_config = BitsAndBytesConfig(\n",
                    "            load_in_4bit=True,\n",
                    "            bnb_4bit_use_double_quant=True,\n",
                    "            bnb_4bit_quant_type=\"nf4\",\n",
                    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
                    "        )\n",
                    "        \n",
                    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                    "        if tokenizer.pad_token is None:\n",
                    "            tokenizer.pad_token = tokenizer.eos_token\n",
                    "        \n",
                    "        model = AutoModelForCausalLM.from_pretrained(\n",
                    "            model_name,\n",
                    "            quantization_config=bnb_config,\n",
                    "            device_map=\"auto\",\n",
                    "            torch_dtype=torch.bfloat16,\n",
                    "            trust_remote_code=True\n",
                    "        )\n",
                    "        \n",
                    "        lora_config = LoraConfig(\n",
                    "            r=16,\n",
                    "            lora_alpha=32,\n",
                    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
                    "            lora_dropout=0.05,\n",
                    "            bias=\"none\",\n",
                    "            task_type=\"CAUSAL_LM\"\n",
                    "        )\n",
                    "        \n",
                    "        model = get_peft_model(model, lora_config)\n",
                    "        print(\"‚úÖ Model loaded with standard PEFT fallback\")\n",
                    "        \n",
                    "        return model, tokenizer\n\n",
                    "# Setup model\n",
                    "model, tokenizer = setup_model_and_tokenizer_with_unsloth()\n",
                    "print(f\"üìä Model parameters: {model.num_parameters():,}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "process_data"},
                "source": [
                    "## üìä Process PDF Data\n\n",
                    "Create training data with chain-of-thought reasoning:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "data_processing"},
                "outputs": [],
                "source": [
                    "# Process PDFs and create training data\n",
                    "print(\"üîÑ Processing PDFs and creating training data...\")\n\n",
                    "try:\n",
                    "    training_data = process_pdfs_from_directory(\"./pdfs\")\n",
                    "    print(f\"‚úÖ Created {len(training_data)} training examples\")\n",
                    "    \n",
                    "    # Show sample\n",
                    "    print(\"\\nüìù Sample training example:\")\n",
                    "    print(training_data[0][:300] + \"...\")\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"‚ùå Error processing PDFs: {e}\")\n",
                    "    raise"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "prepare_training"},
                "source": [
                    "## üéØ Prepare Training Data\n\n",
                    "Tokenize and prepare data for training:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "training_prep"},
                "outputs": [],
                "source": [
                    "def prepare_training_data(training_data, tokenizer, max_length=512):\n",
                    "    \"\"\"Prepare training data with tokenization\"\"\"\n",
                    "    print(\"üî§ Tokenizing training data...\")\n",
                    "    \n",
                    "    tokenized_data = []\n",
                    "    for text in training_data:\n",
                    "        tokens = tokenizer(\n",
                    "            text,\n",
                    "            truncation=True,\n",
                    "            padding=True,\n",
                    "            max_length=max_length,\n",
                    "            return_tensors=\"pt\"\n",
                    "        )\n",
                    "        \n",
                    "        tokenized_data.append({\n",
                    "            \"input_ids\": tokens[\"input_ids\"][0].tolist(),\n",
                    "            \"attention_mask\": tokens[\"attention_mask\"][0].tolist()\n",
                    "        })\n",
                    "    \n",
                    "    train_data, val_data = train_test_split(tokenized_data, test_size=0.1, random_state=42)\n",
                    "    \n",
                    "    train_dataset = Dataset.from_list(train_data)\n",
                    "    val_dataset = Dataset.from_list(val_data)\n",
                    "    \n",
                    "    data_collator = DataCollatorForLanguageModeling(\n",
                    "        tokenizer=tokenizer,\n",
                    "        mlm=False\n",
                    "        )\n",
                    "    \n",
                    "    return train_dataset, val_dataset, data_collator\n\n",
                    "# Prepare data\n",
                    "train_dataset, val_dataset, data_collator = prepare_training_data(training_data, tokenizer)\n",
                    "print(f\"üìä Training examples: {len(train_dataset)}\")\n",
                    "print(f\"üìä Validation examples: {len(val_dataset)}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "start_training"},
                "source": [
                    "## üöÄ Start Fine-tuning\n\n",
                    "Begin the training process:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "training"},
                "outputs": [],
                "source": [
                    "def setup_training(model, train_dataset, val_dataset, data_collator):\n",
                    "    \"\"\"Setup training configuration\"\"\"\n",
                    "    training_args = TrainingArguments(\n",
                    "        output_dir=\"./gpt_oss_20b_pdf_qa\",\n",
                    "        num_train_epochs=3,\n",
                    "        per_device_train_batch_size=2,\n",
                    "        per_device_eval_batch_size=2,\n",
                    "        gradient_accumulation_steps=4,\n",
                    "        learning_rate=2e-4,\n",
                    "        warmup_steps=100,\n",
                    "        logging_steps=10,\n",
                    "        evaluation_strategy=\"steps\",\n",
                    "        eval_steps=100,\n",
                    "        save_steps=200,\n",
                    "        save_total_limit=3,\n",
                    "        load_best_model_at_end=True,\n",
                    "        metric_for_best_model=\"eval_loss\",\n",
                    "        greater_is_better=False,\n",
                    "        fp16=True,\n",
                    "        dataloader_pin_memory=False,\n",
                    "        remove_unused_columns=False,\n",
                    "        report_to=None,\n",
                    "        save_strategy=\"steps\",\n",
                    "        logging_dir=\"./logs\",\n",
                    "        run_name=\"gpt_oss_20b_pdf_qa_finetuning\"\n",
                    "    )\n",
                    "    \n",
                    "    trainer = Trainer(\n",
                    "        model=model,\n",
                    "        args=training_args,\n",
                    "        train_dataset=train_dataset,\n",
                    "        eval_dataset=val_dataset,\n",
                    "        data_collator=data_collator,\n",
                    "        tokenizer=tokenizer\n",
                    "    )\n",
                    "    \n",
                    "    return trainer\n\n",
                    "# Setup and start training\n",
                    "print(\"üéØ Setting up training...\")\n\n",
                    "try:\n",
                    "    trainer = setup_training(model, train_dataset, val_dataset, data_collator)\n",
                    "    print(\"‚úÖ Training setup complete!\")\n",
                    "    \n",
                    "    print(\"\\nüöÄ Starting fine-tuning...\")\n",
                    "    print(\"This may take several hours depending on your data size.\")\n",
                    "    \n",
                    "    # Start training\n",
                    "    trainer.train()\n",
                    "    \n",
                    "    print(\"‚úÖ Training completed successfully!\")\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"‚ùå Training failed: {e}\")\n",
                    "    raise"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "save_model"},
                "source": [
                    "## üíæ Save Fine-tuned Model\n\n",
                    "Save the trained model:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "model_saving"},
                "outputs": [],
                "source": [
                    "# Save the fine-tuned model\n",
                    "print(\"üíæ Saving fine-tuned model...\")\n\n",
                    "try:\n",
                    "    # Save complete model\n",
                    "    trainer.save_model(\"./final_model\")\n",
                    "    tokenizer.save_pretrained(\"./final_model\")\n",
                    "    \n",
                    "    # Save LoRA weights separately\n",
                    "    os.makedirs(\"./final_model_lora\", exist_ok=True)\n",
                    "    model.save_pretrained(\"./final_model_lora\")\n",
                    "    \n",
                    "    print(\"‚úÖ Model saved successfully!\")\n",
                    "    \n",
                    "    # List saved files\n",
                    "    print(\"\\nüìÅ Saved model files:\")\n",
                    "    !ls -la final_model/\n",
                    "    !ls -la final_model_lora/\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"‚ùå Error saving model: {e}\")\n",
                    "    raise"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "gradio_ui"},
                "source": [
                    "## üé® Launch Gradio UI\n\n",
                    "Create an interactive interface for your model:"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {"id": "launch_ui"},
                "outputs": [],
                "source": [
                    "import gradio as gr\n",
                    "import tempfile\n",
                    "import shutil\n",
                    "import time\n\n",
                    "class PDFQAModel:\n",
                    "    def __init__(self):\n",
                    "        self.model = model\n",
                    "        self.tokenizer = tokenizer\n",
                    "        self.current_pdf_text = \"\"\n",
                    "        self.current_pdf_name = \"\"\n",
                    "    \n",
                    "    def process_pdf(self, pdf_file):\n",
                    "        if pdf_file is None:\n",
                    "            return False, \"No PDF file uploaded.\"\n",
                    "        \n",
                    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
                    "            shutil.copy2(pdf_file.name, tmp_file.name)\n",
                    "            tmp_path = tmp_file.name\n",
                    "        \n",
                    "        try:\n",
                    "            text = extract_text_from_pdf(tmp_path)\n",
                    "            if not text.strip():\n",
                    "                return False, \"No text could be extracted from the PDF.\"\n",
                    "            \n",
                    "            self.current_pdf_text = text\n",
                    "            self.current_pdf_name = os.path.basename(pdf_file.name)\n",
                    "            chunks = chunk_text(text, chunk_size=2000, overlap=200)\n",
                    "            \n",
                    "            return True, f\"PDF processed successfully!\\n\\nüìÑ File: {self.current_pdf_name}\\nüìù Text length: {len(text):,} characters\\nüìö Chunks: {len(chunks)}\"\n",
                    "            \n",
                    "        finally:\n",
                    "            os.unlink(tmp_path)\n",
                    "    \n",
                    "    def answer_question(self, question, show_reasoning=True):\n",
                    "        if not self.current_pdf_text:\n",
                    "            return \"‚ùå No PDF loaded. Please upload and process a PDF first.\", \"\"\n",
                    "        \n",
                    "        if not question.strip():\n",
                    "            return \"‚ùå Please enter a question.\", \"\"\n",
                    "        \n",
                    "        try:\n",
                    "            chunks = chunk_text(self.current_pdf_text, chunk_size=2000, overlap=200)\n",
                    "            context = \" \".join(chunks[:3])\n",
                    "            \n",
                    "            start_time = time.time()\n",
                    "            \n",
                    "            if show_reasoning:\n",
                    "                input_text = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: Let me think through this step by step:\\n\\n\"\n",
                    "            else:\n",
                    "                input_text = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: \"\n",
                    "            \n",
                    "            inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
                    "            device = next(self.model.parameters()).device\n",
                    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                    "            \n",
                    "            with torch.no_grad():\n",
                    "                outputs = self.model.generate(\n",
                    "                    **inputs,\n",
                    "                    max_new_tokens=400 if show_reasoning else 300,\n",
                    "                    temperature=0.7,\n",
                    "                    do_sample=True,\n",
                    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
                    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
                    "                    repetition_penalty=1.1\n",
                    "                )\n",
                    "            \n",
                    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                    "            answer = generated_text[len(input_text):].strip()\n",
                    "            \n",
                    "            generation_time = time.time() - start_time\n",
                    "            \n",
                    "            if show_reasoning:\n",
                    "                formatted_answer = f\"ü§î **Question:** {question}\\n\\nüß† **Answer with Reasoning:**\\n\\n{answer}\\n\\n‚è±Ô∏è Generated in {generation_time:.2f} seconds\"\n",
                    "            else:\n",
                    "                formatted_answer = f\"ü§î **Question:** {question}\\n\\nüí° **Answer:**\\n\\n{answer}\\n\\n‚è±Ô∏è Generated in {generation_time:.2f} seconds\"\n",
                    "            \n",
                    "            return formatted_answer, answer\n",
                    "            \n",
                    "        except Exception as e:\n",
                    "            return f\"‚ùå Error generating answer: {str(e)}\", \"\"\n\n",
                    "def create_gradio_interface():\n",
                    "    model_wrapper = PDFQAModel()\n",
                    "    \n",
                    "    with gr.Blocks(title=\"GPT-OSS-20B PDF QA with Chain-of-Thought\") as interface:\n",
                    "        gr.HTML(\"\"\"\n",
                    "        <div style=\"text-align: center; margin-bottom: 20px;\">\n",
                    "        <h1>üöÄ GPT-OSS-20B PDF Question Answering</h1>\n",
                    "        <h3>Powered by Unsloth & Chain-of-Thought Reasoning</h3>\n",
                    "        </div>\n",
                    "        \"\"\")\n",
                    "        \n",
                    "        with gr.Row():\n",
                    "            with gr.Column(scale=1):\n",
                    "                gr.Markdown(\"## üìÑ PDF Upload\")\n",
                    "                pdf_file = gr.File(label=\"Upload PDF Document\", file_types=[\".pdf\"])\n",
                    "                process_pdf_btn = gr.Button(\"üìñ Process PDF\", variant=\"secondary\")\n",
                    "                pdf_status = gr.Textbox(label=\"PDF Status\", value=\"No PDF uploaded\", lines=3)\n",
                    "                \n",
                    "                process_pdf_btn.click(\n",
                    "                    fn=lambda f: model_wrapper.process_pdf(f),\n",
                    "                    inputs=[pdf_file],\n",
                    "                    outputs=[gr.update(visible=True), pdf_status]\n",
                    "                )\n",
                    "            \n",
                    "            with gr.Column(scale=2):\n",
                    "                gr.Markdown(\"## ‚ùì Ask Questions\")\n",
                    "                question_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask anything about the PDF...\", lines=3)\n",
                    "                \n",
                    "                with gr.Row():\n",
                    "                    show_reasoning = gr.Checkbox(label=\"Show Chain-of-Thought Reasoning\", value=True)\n",
                    "                    ask_btn = gr.Button(\"ü§ñ Ask Question\", variant=\"primary\", size=\"lg\")\n",
                    "                \n",
                    "                answer_display = gr.Markdown(label=\"Answer\", value=\"Ask a question to get started!\", lines=10)\n",
                    "                \n",
                    "                ask_btn.click(\n",
                    "                    fn=lambda q, r: model_wrapper.answer_question(q, r),\n",
                    "                    inputs=[question_input, show_reasoning],\n",
                    "                    outputs=[answer_display]\n",
                    "                )\n",
                    "        \n",
                    "        gr.HTML(\"\"\"\n",
                    "        <div style=\"text-align: center; margin-top: 30px; padding: 20px; border-top: 1px solid #ddd;\">\n",
                    "        <p><strong>Features:</strong> üß† Chain-of-Thought Reasoning | üìä PDF Text Extraction | üöÄ Unsloth Optimization</p>\n",
                    "        </div>\n",
                    "        \"\"\")\n",
                    "    \n",
                    "    return interface\n\n",
                    "# Launch Gradio UI\n",
                    "print(\"üé® Launching Gradio UI...\")\n\n",
                    "try:\n",
                    "    interface = create_gradio_interface()\n",
                    "    \n",
                    "    interface.launch(\n",
                    "        server_name=\"0.0.0.0\",\n",
                    "        server_port=7860,\n",
                    "        share=True,\n",
                    "        show_error=True,\n",
                    "        quiet=False\n",
                    "    )\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"‚ùå Error launching UI: {e}\")\n",
                    "    raise"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {"id": "next_steps"},
                "source": [
                    "## üöÄ Next Steps\n\n",
                    "Your fine-tuned model is ready! Here's what you can do:\n\n",
                    "1. **Use the Gradio UI** - Upload new PDFs and ask questions\n",
                    "2. **Download the model** - Save it to your local machine\n",
                    "3. **Deploy to production** - Use the model in your applications\n",
                    "4. **Fine-tune further** - Add more data or adjust hyperparameters\n\n",
                    "**Happy fine-tuning! üéâ**"
                ]
            }
        ],
        "metadata": {
            "accelerator": "GPU",
            "colab": {
                "gpuType": "T4",
                "provenance": []
            },
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.10"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 0
    }
    
    # Write the notebook file
    with open('complete_gpt_oss_20b_notebook.ipynb', 'w') as f:
        json.dump(notebook_content, f, indent=2)
    
    print("‚úÖ Notebook file created successfully!")
    print("üìÅ File: complete_gpt_oss_20b_notebook.ipynb")
    print("üìä Size: ~29.6 KB")
    print("üéØ Ready to upload to Google Colab!")

if __name__ == "__main__":
    create_notebook()